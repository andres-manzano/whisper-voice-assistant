import openai
import os
import pyaudio
import speech_recognition as sr
from dotenv import load_dotenv, find_dotenv

# Load .env file that contains the OPENAI_API_KEY
_ = load_dotenv(find_dotenv())
openai.api_key = os.environ['OPENAI_API_KEY']

class VoiceAssistant():
    
    def __init__(self):
        """
        Initializes the voice assistant.
        
        Attributes:
            self.microphone: A microphone instance to capture audio input.
            self.recognizer: A speech recognizer instance to process the captured audio.
            self.player_stream: An audio output stream configured with specific audio settings.
        """
        self.microphone = sr.Microphone()
        self.recognizer = sr.Recognizer()
        self.player_stream = pyaudio.PyAudio().open(format=pyaudio.paInt16, channels=1, rate=24000, output=True)
    
    def detect_speech(self):
        """
        Detects and recognizes speech using the Whisper API, continuously listening until speech is detected or an error occurs.
        
        Returns:
            str or bool: The recognized text if successful, or False if no speech is detected or an error occurs.
        """
        while True:
            try:
                with self.microphone as source:
                    # Adjust the recognizer sensitivity to ambient noise
                    self.recognizer.adjust_for_ambient_noise(source, duration = 0.5)
                    # Signal to the user that the system is ready to listen.
                    print("I'm listening...")
                    # Listen the phrases and extract it into audio data.
                    audio = self.recognizer.listen(source, timeout = 6.5)
                    # Speech-to-Text using the Whisper API.
                    text = self.recognizer.recognize_whisper_api(audio)
                    
                    return text
                
            except sr.RequestError as e:
                # Handle errors from the Whisper API request.
                print(f"Could not request results from Whisper API: {e}")
                
            except sr.UnknownValueError:
                # Handle the case where speech was detected but not recognized.
                print("Unknown error occurred.")
            
            except sr.WaitTimeoutError:
                # Handle the case when no speech is detected within the timeout.
                print("No speech detected within the timeout period.")
                
                return False
    
    def get_completion(self, messages, model = "gpt-4o", temperature=0):
        """
        Generates a response from the provided messages using OpenAI's GPT model.
        
        Parameters:
            messages: A list of message dictionaries where each dictionary represents a single turn of conversation with keys 'role' and 'content'.
            model: The model ID of the OpenAI GPT model to use for generating responses.
            temperature: Controls the randomness of the response generation.
            
        Returns:
            str: The content of the response message generated by the AI.
        """
    # Call OpenAI's API to generate a chat completion response based on the input messages.
        response = openai.chat.completions.create(
            model = model,
            messages = messages,
            max_tokens = 1500,
            temperature = temperature
        )
        # Extract the content from the response and append it back to the messages.
        message = response.choices[0].message.content
        messages.append({"role": "assistant", "content": message})
        
        return message
    
    def speak(self, input_text):
        """
        Converts input text to spoken audio using OpenAI's TTS (text-to-speech) API and outputs it through an audio stream.
        
        Parameters:
            input_text (str): The text that needs to be converted to speech.
        
        The audio output is handled by PyAudio, which plays the speech through the default audio device (e.g., speakers).
        Each piece of the audio is processed in chunks to ensure smooth playback without requiring the entire audio clip to
        be loaded into memory at once.
        """
        # Reference to the player stream object for outputting audio
        player_stream = self.player_stream
        
        # Create a streaming response from OpenAI's TTS API configured with specific settings
        with openai.audio.speech.with_streaming_response.create( 
            model="tts-1", 
            voice="onyx", 
            response_format="pcm", 
            input=input_text, 
        ) as response:
            # Iterate over each chunk of audio data returned by the API and write it to the audio stream
            for chunk in response.iter_bytes(chunk_size=1024): 
                player_stream.write(chunk)

# Main execution for the voice assistant application.
# Instance of the VoiceAssistant class.
v_assist = VoiceAssistant()
# Initialize the conversation with the system's role and instructions for the assistant's behavior.
messages = [{'role': 'system', 'content': "Your responses should be friendly, helpful, concise, and include elements of wit and technical expertise. Adapt your tone based on the user's mood and the context of the conversation."}]

try:
    # Continuously process input speech until a stop command is detected.
    while True:
        # Detect speech using the voice assistant's method. It listens and returns the text spoken.
        is_speaking = v_assist.detect_speech()
        # Check if the detected speech is a command to stop the assistant.
        if is_speaking == "Stop.":
            break   # Break the loop and end the process if "Stop." is spoken.
        
        # Output to the console that the system is processing the detected speech.
        print("Processing speech...")
        # Display the recognized speech text.
        print(is_speaking)
        
        # Append the recognized speech to the messages list with the user's role.
        messages.append({'role': 'user', 'content': is_speaking})
        # Generate a response based on the conversation history.
        response = v_assist.get_completion(messages, temperature = 0.5)
        # Speak out the generated response.
        v_assist.speak(response)
        # Output the response to the console for monitoring.
        print(response)
        
finally:
    # Ensure that a final message is printed when the assistant stops running.
    print("Assistant stopped.")